---
title: "Rating Scale Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Norms (raw-to-T) lookup tables

This code template derives normalized T-scores from an input data file with multiple raw scores, including subscales and composite scores. Because the optimal normalization model may vary with different raw score distributions, the template processes only a single input file (i.e., data from a single rating-scale form, identified by a combination of age-range (e.g., "child") and rater (e.g., "parent"). For projects with multiple forms that require separate raw-to-T lookup tables, the recommended approach is to implement the template separately for each input file.

Output includes raw-to-T lookup tables in both basic and print formats, as well as tables of demographic counts and raw score descriptive statistics. 

<br>

#### 1. Load packages, read data, initialize tokens for robust code

###### EXECUTABLE CODE
```{r norms-load, eval = FALSE}
suppressMessages(library(here))
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(library(psych))
suppressMessages(library(bestNormalize))

urlRemote_path  <- "https://raw.githubusercontent.com/"
github_path <- "DSHerzberg/RATING-SCALE-ANALYSIS/master/INPUT-FILES/"
input_name <- "data-RS-sim-child-parent.csv"

item_prefix <- "cp"
scale_prefix <- "CP"
scale_suffix <- c("S1", "S2", "S3", "S4", "S5", "TOT")
age_range_name <- "child"
form_name <- "parent"
all_raw_range <- 10:200
TOT_raw_lower_bound <- 50
subscale_raw_upper_bound <- 40
t_score_lower_bound <- 40
t_score_upper_bound <- 80

assign(
  str_c("data", age_range_name, form_name, sep = "_"),
  suppressMessages(read_csv(url(
    str_c(urlRemote_path, github_path, input_name)
  ))) %>%
    mutate(across(
      contains(str_c(item_prefix, "i")),
      ~ case_when(
        .x == "never" ~ 1,
        .x == "occasionally" ~ 2,
        .x == "frequently" ~ 3,
        .x == "always" ~ 4
      )
    ))
)
```

<br>

###### COMMENTED SNIPPETS
Load packages for file path specification (`here`), data wrangling (`tidyverse`),  psychometric data simulation and analysis (`psych`), and normalization of raw score distributions (`bestNormalize`).
```{r norms-load, echo = 1:4, eval = F}
```
Ideally, code templates are _robust_, meaning that they can be adapted to different projects with minimal copying-and-pasting of text. Robust templates are characterized by their liberal use of _tokens_, which are names for _project-specific_ data elements that are used repeatedly in the template.

These tokenized elements include names of variables, files, test forms, raters, etc., as well as labels for age- and score ranges, score item counts, score numerical bounds, and so forth. By definition, these elements differ between, say, an autism rating-scale project and an ADHD rating-scale project. 

In contrast, the _project-general_ aspects of an R script are those functions and operations whose specification is identical for all projects (e.g., creating a table of descriptive statistics requires calling the same set of functions, regardless of whether the input data are from the autism rating scale or the ADHD rating scale). In a robust template, tokens representing project-specific elements are defined at the head of the script. The remainder of the script, therefore, consists only of project-general code.

In R, we initialize tokens as vectors. In the next code snippet, for example, `item_prefix <- "cp"` defines a vector that holds a prefix used in the names of all columns containing item responses. The snippet also includes tokens for reading a certain input file from a certain remote URL, among other project-specific elements.
```{r norms-load, echo = 5:19, eval = F}
```
Many analytic procedures require the specification of an entire set of project-specific elements, such as the entire set of item names, or form names. We can use string functions to combine tokens and explicit text as needed, both to create the names of the elements within the set, and to create the accompanying token names. For instance, we can assemble a vector containing all of the item names by concatenating the token `item_prefix` with a numerical series (e.g., `c("001", "002", "003")`). 

Often, the best token name for the resulting set of elements is itself a concatenated string. To use a concantenated string as the name of an object in R, we cannot rely on the conventional assignment operator `<-`. Instead, we use `base::assign()` to initialize these concatenated names. `assign()` takes two arguments, the desired token name (concatenated with `stringr::str_c()`), and the code snippet that defines the object(s) to be named.

As an example, `str_c("data", age_range_name, form_name, sep = "_")` returns the name `data_child_parent`, combining the file type description (`"data"`, provided as an explicit string) with the age-range (`age_range_name`) and rater (`form_name`) vectors. The combination of these latter two vectors identifies the form whose data is contained in the input file
```{r norms-load, echo = 21:22, eval = F}
```
The next snippet contains the second argument of `assign()`, which defines the data object to be named with the concatenated string. Here, an object is created by reading the input data file with `readr::read_csv()`, and recoding the item responses with `dplyr::mutate()`. Note how the input file path is a concatenation of three previously defined tokens `str_c(urlRemote_path, github_path, input_name)`.

Within `mutate()`, we use `across()` to identify and process a subset of columns. `across()` takes two arguments: the column subset, and a function to be applied to those columns. In this example, we subset the item response columns with the `tidyselect` helper `contains()`, providing as its argument a concatentated string that appears _only_ in the names of the item response columns. We use `case_when()` to conditionally recode the cell values (`.x`) of these columns, converting the input strings to numbers (e.g., `"never" ~ 1`).
```{r norms-load, echo = 23:33, eval = F}
```

<br>

#### 2. Determine optimal normalization model and calcuate normalized T-scores per case

###### EXECUTABLE CODE
```{r norms-model, eval = FALSE}
assign(str_c("data", age_range_name, form_name, "TOT", sep = "_"),
       suppressMessages(read_csv(url(
         str_c(urlRemote_path, github_path, input_name)
       ))) %>% 
         select(!!sym(str_c(scale_prefix, "TOT_raw"))) %>% 
         as_vector() %>% 
         set_names(NULL)
)

set.seed(12345)
TOT_nz_obj <- bestNormalize(data_child_parent_TOT)
TOT_nz_obj$chosen_transform
chosen_transform <- class(TOT_nz_obj$chosen_transform)

raw_score_vecs_list <-
  map(str_c(scale_prefix, scale_suffix),
      ~ get(str_c("data", age_range_name, form_name, sep = "_")) %>%
        pull(!!sym(str_c(.x, "_raw")))) %>% 
  set_names(str_c(scale_prefix, scale_suffix, "_raw"))

nzScore_perCase <- raw_score_vecs_list %>% 
  map(~ get(chosen_transform)(.x)) %>% 
  map(~ tibble(pluck(.x, "x.t"))) %>% 
  bind_cols() %>%
  set_names(str_c(scale_suffix, "_nz")) 

ntScore_perCase <- nzScore_perCase %>%
  mutate(across(everything(),
                ~
                  (round(. * 10) + 50) %>%
                  {
                    case_when(
                      . < t_score_lower_bound ~ t_score_lower_bound,
                      . > t_score_upper_bound ~ t_score_upper_bound,
                      TRUE ~ .
                    )
                  } %>%
                  as.integer)) %>%
  rename_with( ~ str_c(scale_prefix, str_replace_all(., "nz", "nt")))

assign(
  str_c("data", age_range_name, form_name, "nt", sep = "_"),
  get(str_c("data", age_range_name, form_name, sep = "_")) %>% bind_cols(ntScore_perCase) %>%
    mutate(clin_status = 'typ',
           clin_dx = NA) %>%
    select(
      ID:region,
      clin_status,
      clin_dx,
      contains("raw"),
      contains("nt"),
      everything()
    )
)

write_csv(get(str_c(
  "data", age_range_name, form_name, "nt", sep = "_"
)),
here(str_c(
  "OUTPUT-FILES/TABLES/",
  str_c("nt-Scores-per-case",
        age_range_name,
        form_name,
        sep = "-"),
  ".csv"
)),
na = '')

get(str_c("data", age_range_name, form_name, "nt", sep = "_")) %>%
  select(contains("TOT_nt")) %>%
  as_vector() %>%
  MASS::truehist(.,
                 h = 1,
                 prob = FALSE,
                 xlab = "TOT_nt")
```
###### COMMENTED SNIPPETS
The `bestNormalize` package provides tools for determining an optimal normalization model and applying that model to a raw score distribution. For the latter step, the `bestNormalize()` function requires as input the raw score distribution formatted as a numerical vector.

As in previous steps, we use `assign()` to name objects with concatenated strings. To create the numerical vector required by `bestNormalize()`, we `select()` the column containing the raw total scores for (`TOT_raw`). It is best to use a total or composite score for this purpose, because by definition a total score captures more of the sample variance than a subscale score that consists of fewer items. `select()` requires unquoted column names as input, which we provide by wrapping the concatenated column name (the quoted string returned by `str_c()`) in the unquoting function `!!sym()`.

After the `select()` line, the data object is a single-column data frame that holds the `TOT_raw` scores from the input data set. We use two functions from `purrr` to return the requisite vector formatting: `as_vector()`, which returns a named vector, and `set_names(NULL)`, which strips out the name. The raw score distribution vector `data_child_parent_TOT` is now ready for processing with `bestNormalize()`.
```{r norms-model, echo = 1:8, eval = F}
```
To ensure that `bestNormalize()` chooses the same normalizing function every time the script is run, we call `base::set.seed()` to lock down the output of R's random number generator. Calling `bestNormalize(data_parent_child_TOT)` returns a list, with the element `TOT_nz_obj$chosen_transform` holding the name of the selected normalizing function as a class attribute. To use that function in downstream code, we extract its name with `base::class()`, which returns the class attribute(s) of a list element.
```{r norms-model, echo = 10:13, eval = F}
```
Next we apply the chosen normalization function iteratively to all of the raw score distributions in the input file, including total and subscale scores. As a reminder, the normalization functions require raw score distributions to be formatted as numerical vectors (rather than data frame columns).

The next snippet extracts those vectors from the input file and places them in a list `raw_score_vecs_list`. The core function is `dplyr::pull()`, which extracts a column from a data frame and returns it as a vector. In this example, we have six raw score columns to extract, so we use `purrr::map()` to iterate over a vector that holds the six concatenated substrings that identify the target columns `str_c(scale_prefix, scale_suffix)`.

Because the name of the input file is a concatenated string `str_c("data", age_range_name, form_name, sep = "_")`, we wrap the string in `base::get()` which returns the input data frame named by the string. This data object is piped into `pull()` which requires an unquoted column name as its argument. `map()` supplies this argument by iterating over the input vector `str_c(scale_prefix, scale_suffix)`. The string elements of this vector are supplied iteratively to the call of `pull()` via the token `.x`, which `str_c()` combines with the explicit string `"_raw"` to yield a target column name. Because it is a quoted string, we need to unquote by wrapping it in `!!sym()`.

After `map()` finishes iterating, the data object is a list holding six numerical vectors (the raw score distributions). We use `set_names()` to name these elements for downstream processing.
```{r norms-model, echo = 15:19, eval = F}
```
Now we use two sequential `map()` calls to apply the chosen normalizing function to each raw score distribution, yielding a normalized z-score for each case (row). In the next snippet, we initiate the pipeline with the list of vectors to be processed `raw_score_vecs_list`. The first `map()` call iterates over this list and applies the normalizing function. We use `get()` to call the function named by `chosen_transform` and supply the `.x` token as its argument, enabling `map()` to apply the function one vector at a time. After this `map()` call finishes iterating, the data object is a list containing six `bestNormalize` normalization objects, each one itself a list named for the raw score distribution (e.g., `CPS1_raw`) whose normalized output it contains.

These normalization objects contain an element `x.t.`, which holds the distribution of normalized z-scores per case. To extract this element, we call `purrr::pluck()`, and supply as the first argument the `.x` token, which represents the currently-iterated normalization object, and as the second argument `"x.t."`, which names the desired element. `pluck()` returns the z-score distribution as a numerical vector. We format the vector as a single-column data frame by wrapping the `pluck()` call in `tibble::tibble()`. Note the difference between `pull()`, which extracts a column from a data frame, and `pluck()`, which extracts an element from a list.

The data object is now a list of six single-column data frames, which we bind into a single data frame with `dplyr::bind_cols`. We complete the operation by applying desired column names with `set_names()`. The resulting output is a six-column data frame containing the distributions of normalized z-scores corresponding to the six raw score distributions.
```{r norms-model, echo = 21:25, eval = F}
```
The next step is to convert the z-scores to T-scores, and to truncate the T-score distribution according to the project parameters (represented in the tokens `t_score_lower_bound` and `t_score_upper_bound`). The input here is `nzScore_perCase`, the data frame containing the z-score distributions.

We use `mutate()` to carry out the these transformations. Because `nzScore_perCase` consists only of the six z-score columns, we can use `across(everything())` to apply a series of functions to all columns. In the snippet below, we convert the z-scores to T-scores with `(round(. * 10) + 50)`, truncate the resulting T-score distribution with `case_when()`, and format all values `as.integer()`. We supply desired concatenated column names with `dplyr::rename_with()`.

Note that the`case_when()` call is enclosed in curly braces `{}`. This approach enables control over how `.` (the token representing the data object) is evaluated within a pipeline containing functions that subset `.`. In the code below, `case_when()` subsets the data object with predicates, executing a different transformation when `. < t_score_lower_bound` returns `TRUE` than when `. > t_score_upper_bound` returns `TRUE`. This behavior can cause problems at certain positions within a pipeline. In these instances, the pipe operator `%>%` injects the intact data object as an argument to `case_when()`, replacing the predicate-segmented data that `case_when()` expects. This throws an error. Wrapping the `case_when()` call in curly braces prevents this substitution, thereby allowing `case_when()` to subset `.` and operate in its usual fashion.
```{r norms-model, echo = 27:39, eval = F}
```
Now we can assemble a table that contains, for each person, the scale-wise raw scores and normalized T-scores. As in previous steps, we use `assign()` to name objects with concatenated strings, and `get()` to initiate the pipeline with a data object named with a concatenated string. In this case, we bind the table named by `str_c("data", age_range_name, form_name, sep = "_")`, which contains the raw score columns, to `ntscore_perCase`, which contains the T-score columns, by calling `dplyr::bind_cols()`, which joins two data frames side-by-side, without indexing them on a shared variable. We then add two new columns with `mutate()`, choose columns to keep, in the desired sequence, with `select()`, and `write_csv()` the output table of T-scores per case to .csv.
```{r norms-model, echo = 41:67, eval = F}
```
As an optional procedure, the script includes a snippet to plot a histogram of the distribution of normalized T-scores for the total score (e.g., `TOT_nt`). This allows a visual check on normality.

The input to the pipeline is the data frame containing normalized T-scores per case, which we access with `get(str_c("data", age_range_name, form_name, "nt", sep = "_"))`. We then `select()` only the column containing the distribution of `TOT_nt`, and use `as.vector()` to transform it from a data frame column into a numeric vector, as required for input to the plotting function.

We use `MASS::truehist()` to generate the plot, passing as arguments bin width `h`, `prob = FALSE` so that the y-axis scale is person counts, as opposed to relative frequency density, and a label `xlab` for the x-axis scale showing that it depicts values of `TOT_nt`.
```{r norms-model, echo = 69:75, eval = F}
```

<br>

#### 3. Generate raw-to-T lookup tables

The lookup relationship between raw and T-scores is _many-to-one_, meaning that each value of raw maps onto one and only one value of T, but each value of T _may_ map onto more than one value of raw.

The next code section produces two raw to T-score lookup tables.

* __Basic format__: The left-most column contains all possible values of raw, across all scales, sorted ascending. Rightward columns contain corresponding T-scores for each scale.
* __Print format__: The left-most column contains all possible values of T (reflecting any truncation of the T-score distribution), sorted descending. Rightward columns contain corresponding raw scores for each scale. Reflecting the many-to-one lookup relationship, some of the rightward cells contain single raw scores, while others contain a range of raw scores.

###### EXECUTABLE CODE
```{r norms-lookup, eval = FALSE}
all_lookup_basic <- map(
  scale_suffix,
  ~ get(str_c(
    "data", age_range_name, form_name, "nt", sep = "_"
  )) %>%
    group_by(!!sym(str_c(
      scale_prefix, .x, "_raw"
    ))) %>%
    summarize(!!sym(str_c(
      scale_prefix, .x, "_nt"
    )) := min(!!sym(
      str_c(scale_prefix, .x, "_nt")
    ))) %>%
    complete(!!sym(str_c(
      scale_prefix, .x, "_raw"
    )) := all_raw_range) %>%
    fill(!!sym(str_c(
      scale_prefix, .x, "_nt"
    ))) %>%
    fill(!!sym(str_c(
      scale_prefix, .x, "_nt"
    )),
    .direction = "up") %>%
    rename(raw = !!sym(str_c(
      scale_prefix, .x, "_raw"
    ))) 
) %>%
  reduce(left_join,
         by = 'raw') %>% 
mutate(across(
  contains(scale_suffix[-length(scale_suffix)]),
  ~ case_when(raw > subscale_raw_upper_bound ~ NA_integer_,
              TRUE ~ .x)
),
across(
  contains(scale_suffix[length(scale_suffix)]),
  ~ case_when(raw < TOT_raw_lower_bound ~ NA_integer_,
              TRUE ~ .x)
))

write_csv(all_lookup_basic,
here(str_c(
  "OUTPUT-FILES/TABLES/",
  str_c("raw-T-lookup",
        age_range_name,
        form_name,
        sep = "-"),
  ".csv"
)),
na = '')

all_lookup_print <- all_lookup_basic %>% 
pivot_longer(contains("nt"), names_to = "scale", values_to = "NT") %>% 
  arrange(scale) %>% 
  group_by(scale) %>%
  complete(NT = 40:80) %>% 
  ungroup() %>%
  group_by(scale, NT) %>%
  filter(n() == 1 | n() > 1 & row_number()  %in% c(1, n())) %>%
  summarize(raw = str_c(raw, collapse = '--')) %>%
  mutate(across(raw, ~ case_when(is.na(.x) ~ '-', TRUE ~ .x))) %>%
  arrange(scale, desc(NT)) %>% 
  pivot_wider(names_from = scale,
              values_from = raw) %>% 
  arrange(desc(NT)) %>% 
  rename_with(~ str_replace_all(., "_nt", "_raw")) %>%
  rename(T_score = NT) %>% 
  filter(!is.na(T_score))

write_csv(all_lookup_print,
          here(str_c(
            "OUTPUT-FILES/TABLES/",
            str_c("raw-T-lookup-print",
                  age_range_name,
                  form_name,
                  sep = "-"),
            ".csv"
          )),
          na = '')
```
###### COMMENTED SNIPPETS
To create a basic-format lookup table for all scales, we start by creating individual two-column looking tables, one for each scale. We can use an interative process powered by `map()` and return a list containing the required two-column data frames.

We pass the `scale_suffix` vector as the first argument to `map()`, as this vector holds the six scale identifiers. The second argument for `map()` is a pipeline of functions, set off with the formua shorthand `~`, to be applied iteratively to the elements of `scale_suffix`. We initiate the pipeline with the data frame containing raw scores and normalized T-scores per case, which we access with `get(str_c("data", age_range_name, form_name, "nt", sep = "_"))`.
```{r norms-lookup, echo = 1:5, eval = F}
```
Recall that in the basic-format lookup table, all possible raw score values appear in the left-most column. We use `group_by()` and `summarize()` to begin assembling this column, passing concatenated score names as arguments to both of these functions. 

As an example, `str_c(scale_prefix, .x, "_raw")` returns a single score name, the one corresponding to the currently-iterated element (`.x`) of `scale_suffix`, the input vector to `map()`. `str_c()` inserts this element between `scale_prefix` (a single-element token) and `"_raw"` (static text), and returns the resulting concatenated string (e.g., `"CPS1_raw"`). In order to use a string as a column name within a `dplyr` function, we must unquote it by wrapping `str_c()` with `!!sym()`.

In the code below, the call of `group_by()` groups the data by raw score. At this point, the data object includes all rows in the original input file, meaning the raw score column has many duplicate values. To create a lookup table, we need the raw score column to contain a single value for each possible raw score, with no duplicates.  

To remove the duplicate raw score rows, we can call `summarize()`, which aggregates multiple rows of grouped data into a single row, based on a summary function. Here we can take advantage of the many-to-one relationship of raw-to-T, which means that each raw score corresponds to one and only one T-score.  Thus, for example, the data object may contain multiple rows were raw score is 10 and T-score is 50. We need only one of these rows in the output. 

We can use the summary function `min()` to return the only the rows that contain the minimum T-score associated with each raw score, which by definition is also the _only_ T-score associated with that raw score. In the example, `min()` summarizes the group of rows that contain the raw score of 10 by returning only one of these rows, the one containing a T-score of 50, which is the _minimum_ T-score present in _that_ group of rows.

After applying `summarize()` the data object contains a single row for _each raw score value present in the input data file_ (but not yet all possible raw score values). The T-score value associated with each raw score value appears in the same row. Note that within `summarize()`, we use unquoted strings on both sides of an equals sign. Because of this, we need to use the non-standard evaluation (NSE) operator `:=`, instead of a conventional equals sign, in the equation that defines the summary variable (which includes the concatenated string naming the T-score column).
```{r norms-lookup, echo = 6:13, eval = F}
```
In the next snippet, we use `tidyr::complete()` to add rows for possible raw score values that are not present in the input data file. The total range of possible raw score values is available in `all_raw_range`, a token defined earlier. As an argument to `complete()`, we pass an equation whose LHS is the concatenated string naming the raw score column as the variable to be expanded with new values/rows. We use the `:=` operator and the RHS of `all_raw_range`, which enumerates the raw score values _missing_ from the input data file, allowing `complete()` to add rows for them.
```{r norms-lookup, echo = 14:16, eval = F}
```
